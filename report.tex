\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{blindtext}
\usepackage{biblatex}
\usepackage{graphicx}
\title{Project 1}
\author{Espen Ulset Nordsveen
\and Ghadi Al Hajj}
\begin{document}
\maketitle
\section{Introduction}
Linear regression is an important tool within the field of statistics and data analysis. This statistical method aims to determine the strength and character of the relationship between one dependent variable, also called the outcome, and one or more independent variables, also called the features. This is achieved by mapping this relationship through a mathematical formula that best approximates the data points within a given data set.

This tool is used in several fields of science spanning medicine, physics, finance, astronomy...

In this project, our goal was to analyse and compare the performance of various linear regression methods, namely the Ordinary Least Squares (OLS), Ridge regression and the Lasso regression. These methods were combined with resampling techniques like the bootstrap and cross validation techniques to get better estimates of the error, specifically on the test data. In addition, bootstrap was used to perform a bias-variance tradeoff analysis for all methods.

\subsection{Linear Regression}
As mentioned, linear regression attemps to find a relationship between two or more variables by fitting a linear equation to observed data. We denote the dependent variable, or outcome, as $\textbf{y}$. If we have a set of independent variables, or features, $\textbf{X}^{T} = (X_{1}, X_{2}, \dots, X_{p})$, and assuming a linear relationship between these, the $y$ can be predicted as
\begin{equation} \label{ytilde}
\tilde{y} = \tilde{\beta_{0}} + \sum_{j=1}^{p} X_{j} \tilde{\beta_{j}}
\end{equation}
By including the constant variable 1 in $X$, and include $\tilde{\beta_{0}}$ in the vector of coefficients $\tilde{\beta}$,  (\ref{ytilde}) can be written in vector form as
\begin{equation}
\textbf{y} = \textbf{X} \beta + \epsilon
\end{equation}
where $\epsilon$ denotes the residual error of the linear model $\textbf{X}\beta$ from the true response, and the $\beta$ is the parameter vector containing the linear regression coefficients $\beta_{i}$. $y$, $\textbf{X}$ and $\epsilon$ are defined as the following vectors
\begin{equation}
\textbf{y} = [y_{0}, y_{1}, y_{2}, \dots, y_{n-1}]^{T}
\end{equation}

\begin{equation}
\beta = [\beta_{0}, \beta_{1}, \beta_{2}, \dots, \beta_{n-1}]^{T}
\end{equation}

\begin{equation}
\epsilon = [\epsilon_{0}, \epsilon_{1}, \epsilon_{2}, \dots, \epsilon_{n-1}]^{T}
\end{equation}
and $\beta$ are in fact the unknown parameters in the linear regression problems that needs to be calculated.
\begin{equation}
\textbf{\~{y}} = \textbf{X}\beta + \epsilon
\end{equation}

The design matrix
\begin{equation} \label{designM}
\textbf{X} = 
\begin{bmatrix}
1 & x_{0}^{1} & x_{0}^{2} & \dots & x_{0}^{n-1} \\
1 & x_{1}^{1} & x_{1}^{2} & \dots & x_{1}^{n-1} \\
1 & x_{2}^{1} & x_{2}^{2} & \dots & x_{2}^{n-1} \\
\vdots & \vdots & \vdots & \dots & \vdots \\
1 & x_{n-1}^{1} & x_{n-1}^{2} & \dots & x_{n-1}^{n-1} \\ 
\end{bmatrix}
\end{equation}

\subsection{Ordinary Least Squares}
\begin{equation}
C(\textbf{X},\beta) = \dfrac{1}{n} \{( \textbf{y}-\textbf{X}\beta^{T}\textbf{y}-\textbf{X}\beta)\}
\end{equation}
\subsection{Ridge Regression}
\begin{equation}
C(\textbf{X},\beta) = \dfrac{1}{n} \{( \textbf{y}-\textbf{X}\beta^{T}\textbf{y}-\textbf{X}\beta)\} + \lambda \beta^{T} \beta 
\end{equation}
\subsection{Lasso Regression}
\subsection{The bootstrap}
The boostrap resampling method is a simple Monte Carlo technique to approximate the sampling distribution (KILDE). The method randomly draw $B$ datasets with replacement from the training data, where each sample is the same size as the original training set. The model is then refit to the $B$ bootstrap datasets. In the context of this project, we use this method to calculate MSE, bias and variance of the model.
\subsection{k-fold Cross Validation}
The $k$-fold Cross Validation technique is a resampling procedure used to evaluate the performance of statistical models on a limited data sample. The method generally results in a less biased or less optimistic estimate of the model compared to other methods, such as the train/test split. The \textit{k} refers to the equally number of groups the given sample shall be split into. One of these groups are set aside as the validation group. The rest of the groups, i.e. $K-1$, are used to fit the model, and calculate the prediction error of the fitted model when predicting the $k$th part of the data. A normal value for $k$ is $K=5$, as this gives a lower variance. It might, however, give problems with the bias if the training sets becomes too small.

The general procedure for the $k$-fold Cross Validation is as follows (KILDE)
\begin{enumerate}
\item Shuffle the dataset
\item Split the dataset into $k$ groups
\item For each unique group
\begin{enumerate}
\item Take the group as a hold out or test data set
\item Take the remaining groups as a training data set
\item Fit a model on the training set and evaluate it on the test set
\item Retain the evaluation score and discard the model
\end{enumerate}
	
\item Summarize the skill of the model using the sample of model evaluation scores
\end{enumerate}

\section{Data}
\subsection{Franke's function}
The Franke's function will be used as a basis for our discussion of the various regression and resampling methods in this context. This is a function  widely used as a test function in interpolation problems.

\begin{align*}
f(x,y) &= \frac{3}{4}exp\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)+\frac{3}{4}exp\left(-\frac{(9x+1)^2}{49} - \frac{(9y+1)}{10}\right)\\
&+ \frac{1}{2}exp\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)-\frac{1}{5}exp\left(-(9x-4)^2 - (9y-7)^2\right)
\end{align*}
\section{Method}
The Franke function is a multivariable function, depending on both $x$ and $y$, and hence is a 2-variable polynomial. The design matrix in \ref{designM} thus needs to be adjusted for this multivariable case. The 
$$\textbf{X} = \begin{bmatrix}
1 & x_{0} & y_{0} & x_{0}y_{0} & \dots & x_{0}^{N} & y_{0}^{N}     \\
1 & x_{0} & y_{1} & x_{0}y_{1} & \dots & x_{0}^{N} & y_{1}^{N}  \\
\vdots & \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\
1 & x_{1} & y_{0} & x_{1}y_{0} & \dots & x_{1}^{N} & y_{0}^{N} \\
\vdots & \vdots & \vdots & \vdots & \dots & \vdots & \vdots \\
1 & x_{n-1} & y_{m-1} & x_{n-1}y_{m-1} & \dots & x_{n-1}^{N} & y_{m-1}^{N} \\
\end{bmatrix}$$
\section{Results}
\subsection{Ordinary least squares on the Franke Function}
\includegraphics[scale=0.5]{confidenceintervall.png}
\centering

\subsection{Bias-variance trade-off and resampling techniques}
\includegraphics[scale=0.5]{partBplot.jpg}
\centering

\subsection{Bootstrap}
\includegraphics[scale=1]{boostrapridge0.png}
\includegraphics[scale=1]{boostrapridge1.png}
\end{document}

