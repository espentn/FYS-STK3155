\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{blindtext}
\title{Project 1}
\author{Espen Ulset Nordsveen
\and Ghadi Al Hajj}
\begin{document}
\maketitle
\section{Introduction}
Linear regression is an important tool within the field of statistics and data analysis. This statistical method aims to determine the strength and character of the relationship between one dependent variable, also called the outcome, and one or more independent variables, also called the features. This is achieved by mapping this relationship through a mathematical formula that best approximates the data points within a given data set.

This tool is used in several fields of science spanning medicine, physics, finance, astronomy...

In this project, our goal was to analyse and compare the performance of various linear regression methods, namely the Ordinary Least Squares (OLS), Ridge regression and the Lasso regression. These methods were combined with resampling techniques like the bootstrap and cross validation techniques to get better estimates of the error, specifically on the test data. In addition, bootstrap was used to perform a bias-variance tradeoff analysis for all methods.
$$\sum_{n=1}^{N} \beta\textbf{X} $$
\subsection{Linear Regression}
\begin{equation}
\textbf{y} = \textbf{X}\beta + \epsilon
\end{equation}
where $\epsilon$ denotes the deviation of the linear model $\textbf{X}\beta$, and the $\beta$ is the parameter vector containing the linear regression coefficients $\beta_{i}$. $y$, $\textbf{X}$ and $\epsilon$ are defined as the following vectors
\begin{equation}
\textbf{y} = [y_{0}, y_{1}, y_{2}, \dots, y_{n-1}]^{T}
\end{equation}

\begin{equation}
\beta = [\beta_{0}, \beta_{1}, \beta_{2}, \dots, \beta_{n-1}]^{T}
\end{equation}

\begin{equation}
\epsilon = [\epsilon_{0}, \epsilon_{1}, \epsilon_{2}, \dots, \epsilon_{n-1}]^{T}
\end{equation}

\begin{equation}
\textbf{\~{y}} = \textbf{X}\beta + \epsilon
\end{equation}

The design matrix
\begin{equation}
\textbf{X} = 
\begin{bmatrix}
1 & x_{0}^{1} & x_{0}^{2} & \dots & x_{0}^{n-1} \\
1 & x_{1}^{1} & x_{1}^{2} & \dots & x_{1}^{n-1} \\
1 & x_{2}^{1} & x_{2}^{2} & \dots & x_{2}^{n-1} \\
\vdots & \vdots & \vdots & \dots & \vdots \\
1 & x_{n-1}^{1} & x_{n-1}^{2} & \dots & x_{n-1}^{n-1} \\ 
\end{bmatrix}
\end{equation}

\subsection{Ordinary Least Squares}

\subsection{Ridge Regression}
\subsection{Lasso Regression}
\subsection{The bootstrap}
\subsection{k-fold Cross Validation}

\section{Data}
\subsection{Franke's function}
The Franke's function will be used as a basis for our discussion of the various regression and resampling methods in this context. This is a function  widely used as a test function in interpolation problems.

\begin{align*}
f(x,y) &= \frac{3}{4}exp\left(-\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4}\right)+\frac{3}{4}exp\left(-\frac{(9x+1)^2}{49} - \frac{(9y+1)}{10}\right)\\
&+ \frac{1}{2}exp\left(-\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4}\right)-\frac{1}{5}exp\left(-(9x-4)^2 - (9y-7)^2\right)
\end{align*}
\section{Method}
$$\textbf{X} = \begin{bmatrix}
1 & 2 & 3 \\
5 & 6 & 7
\end{bmatrix}$$

\end{document}

